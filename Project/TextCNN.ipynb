{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:53:40.435464Z",
     "start_time": "2019-04-18T02:53:40.406684Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import HybridBlock\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "\n",
    "import d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils as gutils\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "from gluonnlp import Vocab, data\n",
    "from mxnet.gluon.data import ArrayDataset, SimpleDataset\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:39:55.593929Z",
     "start_time": "2019-04-18T02:39:55.573808Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuoraDataset(ArrayDataset):\n",
    "    \"\"\"This dataset provides access to Quora insincere data competition\"\"\"\n",
    "\n",
    "    def __init__(self, segment, root_dir=\"\"):\n",
    "        self._root_dir = root_dir\n",
    "        self._segment = segment\n",
    "        self._segments = {\n",
    "            # We may change the file path\n",
    "            'train': './bert/glue_data/Quora/train.tsv',\n",
    "            'test': './bert/glue_data/Quora/test.tsv'\n",
    "        }\n",
    "\n",
    "        super(QuoraDataset, self).__init__(self._read_data())\n",
    "\n",
    "    def _read_data(self):\n",
    "        file_path = os.path.join(self._root_dir, self._segments[self._segment])\n",
    "        with open(file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "            # ignore 1st line - which is header\n",
    "            data = [tuple(row) for i, row in enumerate(reader) if i > 0]\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i][1:3]\n",
    "                data[i] = list(data[i])\n",
    "                data[i][1] = int(data[i][1])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:40:02.765472Z",
     "start_time": "2019-04-18T02:39:55.596980Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6d1ad0698b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuoraDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-95f1187eda54>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, segment, root_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m         }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuoraDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-95f1187eda54>\u001b[0m in \u001b[0;36m_read_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_dataset = QuoraDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:40:02.779899Z",
     "start_time": "2019-04-18T02:40:02.767956Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_quora(data, vocab):  \n",
    "    max_l = 200  # 将每条评论通过截断或者补0，使得长度变成200\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = d2l.get_tokenized_imdb(data)\n",
    "    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\n",
    "    labels = nd.array([score for _, score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:41:22.602065Z",
     "start_time": "2019-04-18T02:40:02.782963Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data, test_data = nlp.data.train_valid_split(train_dataset,valid_ratio = 0.3)\n",
    "vocab = d2l.get_vocab_imdb(train_data)\n",
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "    *preprocess_quora(train_data, vocab)), batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "    *preprocess_quora(test_data, vocab)), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:41:22.668057Z",
     "start_time": "2019-04-18T02:41:22.604959Z"
    }
   },
   "outputs": [],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:41:22.686820Z",
     "start_time": "2019-04-18T02:41:22.671567Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Dense(2)\n",
    "       \n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        self.convs = nn.Sequential()  \n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeddings = nd.concat(\n",
    "            self.embedding(inputs), self.constant_embedding(inputs), dim=2)\n",
    "        \n",
    "        embeddings = embeddings.transpose((0, 2, 1))\n",
    "        \n",
    "        encoding = nd.concat(*[nd.flatten(\n",
    "            self.pool(conv(embeddings))) for conv in self.convs], dim=1)\n",
    "        \n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:41:22.996664Z",
     "start_time": "2019-04-18T02:41:22.690485Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "ctx = d2l.try_all_gpus()\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T03:17:58.649107Z",
     "start_time": "2019-04-16T03:17:58.616543Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:41:40.130389Z",
     "start_time": "2019-04-18T02:41:23.001384Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_embedding = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.100d.txt', vocabulary=vocab)\n",
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "net.constant_embedding.weight.set_data(glove_embedding.idx_to_vec)\n",
    "net.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T03:57:18.486421Z",
     "start_time": "2019-04-18T03:57:16.341131Z"
    }
   },
   "outputs": [],
   "source": [
    "train_count_1= 0\n",
    "for lis in train_data:\n",
    "    if lis[1] == 1:\n",
    "        train_count_1 += 1\n",
    "\n",
    "test_count_1 = 0\n",
    "for lis in test_data:\n",
    "    if lis[1] == 1:\n",
    "        test_count_1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T03:57:19.150003Z",
     "start_time": "2019-04-18T03:57:19.138112Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_count_1, test_count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T02:53:55.612461Z",
     "start_time": "2019-04-18T02:53:55.606193Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T03:59:08.499933Z",
     "start_time": "2019-04-18T03:59:08.483775Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T03:59:10.538211Z",
     "start_time": "2019-04-18T03:59:10.526276Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_recall(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    recall_sum, n = nd.array([0]),0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            recall_sum += (net(X).argmax(axis=1) == 1).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        recall_sum.wait_to_read()\n",
    "    return recall_sum.asscalar() / test_count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T03:59:18.217521Z",
     "start_time": "2019-04-18T03:59:18.178610Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_acc_recall(train_iter, test_iter, net, loss, trainer, ctx, num_epochs):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, train_recall_sum,n, m, start = 0.0, 0.0, 0.0,0, 0, time.time()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
    "            ls = []\n",
    "            with autograd.record():\n",
    "                print(Xs, ys, batch_size)\n",
    "                assert(0)\n",
    "                y_hats = [net(X) for X in Xs]\n",
    "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
    "            n += sum([l.size for l in ls])\n",
    "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "                                 for y_hat, y in zip(y_hats, ys)])\n",
    "            m += sum([y.size for y in ys])\n",
    "            train_recall_sum += sum([(y_hat.argmax(axis=1) == 1).sum().asscalar()\n",
    "                                 for y_hat, y in zip(y_hats, ys)])\n",
    "            \n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        test_recall = evaluate_recall(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, train recall %.3f, test recall %.3f'\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc, train_recall_sum / train_count_1, test_recall, \n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-18T03:59:21.259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [cpu(0)]\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 15\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_acc_recall(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
