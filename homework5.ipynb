{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n",
    "\n",
    "**Your name: Luyu Chen, SID 3034306110\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/) instead of Github, so you will get the score distribution for each question. Please enroll in the [class](https://www.gradescope.com/courses/42432) by the Entry code: MXG5G5** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(train=True,transform=lambda data, label: (data.astype(np.float32), label))\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False,transform=lambda data, label: (data.astype(np.float32), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 1\n",
    "def loss(y,o):\n",
    "    ## add your loss function here\n",
    "    l = - nd.log(1 + nd.exp(- nd.dot(y, o)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "import matplotlib.pyplot as plt\n",
    "o = nd.arange(-5, 5, 0.01).reshape((1,1000))\n",
    "y = nd.array([1])\n",
    "l = loss(y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "onp = o.asnumpy()[0]\n",
    "plt.plot(onp,-l.asnumpy(), label = 'class of y = 1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('logistic loss function')\n",
    "\n",
    "o.attach_grad()\n",
    "with autograd.record():\n",
    "    func = -loss(y,o)\n",
    "func.backward()\n",
    "plt.plot(onp,o.grad.asnumpy().reshape(1000,),label = 'grad')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(mnist_train, mnist_test,bias, total):\n",
    "    '''\n",
    "    biased ratio is for problem 3\n",
    "    total would be the total sample for each class.\n",
    "    '''\n",
    "    \n",
    "    X, y = mnist_train[:]\n",
    "    coat =  np.where(y == 4)[0].tolist()\n",
    "    shirt = np.where(y == 6)[0].tolist()\n",
    "    sandal = np.where(y == 5)[0].tolist()\n",
    "    sneaker = np.where(y == 7)[0].tolist()\n",
    "    \n",
    "    class1_portion = round(bias * total)\n",
    "    class2_portion = round((1 - bias) * total)\n",
    "    \n",
    "    #for train dataset\n",
    "    #index_coat = random.sample(coat, class1_portion)\n",
    "    #index_shirt = random.sample(shirt, class1_portion)\n",
    "    #index_sandal = random.sample(sandal, class2_portion)\n",
    "    #index_sneaker = random.sample(sneaker, class2_portion)\n",
    "    \n",
    "    index_coat = coat[0:class1_portion]\n",
    "    index_shirt = shirt[0:class1_portion]\n",
    "    index_sandal = sandal[0:class2_portion]\n",
    "    index_sneaker = sneaker[0:class2_portion]\n",
    "    \n",
    "    train_coat = X[index_coat]\n",
    "    train_shirt = X[index_shirt]\n",
    "    train_sandal = X[index_sandal]\n",
    "    train_sneaker = X[index_sneaker]\n",
    "     \n",
    "    train_feature = nd.concat(train_coat,train_shirt, train_sandal, train_sneaker, dim=0)\n",
    "    label1 = nd.ones((1, total)).astype(np.float32)\n",
    "    label2 = nd.zeros((1, total)).astype(np.float32)\n",
    "    \n",
    "    train_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    \n",
    "    #for test dataset    \n",
    "    A, b = mnist_test[:]\n",
    "    indices_1 = np.where(np.logical_or(b == 4, b == 6))[0].tolist() #coat and shirt\n",
    "    indices_2 = np.where(np.logical_or(b == 5, b == 7))[0].tolist() #sandal and sneaker\n",
    "\n",
    "    class1 = A[indices_1] #class1\n",
    "    class0 = A[indices_2] #class-1\n",
    "    test_feature = nd.concat(class1, class0,dim = 0)\n",
    "    label1 = nd.ones((1, 2000)).astype(np.float32)\n",
    "    label0 = nd.zeros((1,2000)).astype(np.float32)\n",
    "    test_label = nd.concat(label1,label0,dim = 1).reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_label)    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l,random\n",
    "train_all, test_all = generate_data(mnist_train, mnist_test,0.5, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train, test, batch_size, lr, num_epochs):\n",
    "    train_iter = gdata.DataLoader(train, batch_size,shuffle = True)\n",
    "    test_iter = gdata.DataLoader(test, batch_size,shuffle = True)\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(2))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1600.7892, train acc 0.985, test acc 0.997\n",
      "epoch 2, loss 29.9104, train acc 0.998, test acc 0.999\n",
      "epoch 3, loss 22.9545, train acc 0.998, test acc 0.999\n",
      "epoch 4, loss 19.6463, train acc 0.999, test acc 0.999\n",
      "epoch 5, loss 16.3328, train acc 0.999, test acc 0.999\n"
     ]
    }
   ],
   "source": [
    "model(train_all, test_all, 256, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train half sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_half, test_half = generate_data(mnist_train, mnist_test,0.5, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 657.3146, train acc 0.966, test acc 0.993\n",
      "epoch 2, loss 108.0598, train acc 0.996, test acc 0.995\n",
      "epoch 3, loss 60.3451, train acc 0.998, test acc 0.997\n",
      "epoch 4, loss 46.4209, train acc 0.998, test acc 0.998\n",
      "epoch 5, loss 37.1828, train acc 0.998, test acc 0.999\n"
     ]
    }
   ],
   "source": [
    "model(train_half, test_half, 256, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $600$ `sweater` images and likewise $5,400$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is 0.05\n",
      "epoch 1, loss 13746.6903, train acc 0.763, test acc 0.797\n",
      "epoch 2, loss 6300.7187, train acc 0.854, test acc 0.796\n",
      "epoch 3, loss 5795.2292, train acc 0.859, test acc 0.770\n",
      "epoch 4, loss 5044.2806, train acc 0.875, test acc 0.747\n",
      "epoch 5, loss 4329.9004, train acc 0.884, test acc 0.761\n",
      "Bias is 0.10\n",
      "epoch 1, loss 14772.2570, train acc 0.773, test acc 0.816\n",
      "epoch 2, loss 8150.8907, train acc 0.815, test acc 0.799\n",
      "epoch 3, loss 7333.8877, train acc 0.830, test acc 0.833\n",
      "epoch 4, loss 6591.5090, train acc 0.840, test acc 0.939\n",
      "epoch 5, loss 7271.4670, train acc 0.845, test acc 0.763\n",
      "Bias is 0.20\n",
      "epoch 1, loss 14084.8478, train acc 0.749, test acc 0.979\n",
      "epoch 2, loss 8517.7672, train acc 0.793, test acc 0.787\n",
      "epoch 3, loss 7954.2751, train acc 0.798, test acc 0.829\n",
      "epoch 4, loss 7248.9983, train acc 0.801, test acc 0.796\n",
      "epoch 5, loss 7526.5245, train acc 0.802, test acc 0.985\n",
      "Bias is 0.30\n",
      "epoch 1, loss 12895.3116, train acc 0.769, test acc 0.719\n",
      "epoch 2, loss 7464.9982, train acc 0.796, test acc 0.902\n",
      "epoch 3, loss 6580.4470, train acc 0.806, test acc 0.906\n",
      "epoch 4, loss 6543.0471, train acc 0.809, test acc 0.957\n",
      "epoch 5, loss 6213.5429, train acc 0.809, test acc 0.861\n",
      "Bias is 0.40\n",
      "epoch 1, loss 6821.2959, train acc 0.835, test acc 0.886\n",
      "epoch 2, loss 3744.4527, train acc 0.855, test acc 0.978\n",
      "epoch 3, loss 3698.2450, train acc 0.857, test acc 0.997\n",
      "epoch 4, loss 3187.7375, train acc 0.864, test acc 0.712\n",
      "epoch 5, loss 3784.9320, train acc 0.865, test acc 0.965\n",
      "Bias is 0.50\n",
      "epoch 1, loss 4696.3024, train acc 0.962, test acc 0.994\n",
      "epoch 2, loss 96.7228, train acc 0.997, test acc 0.997\n",
      "epoch 3, loss 62.5891, train acc 0.998, test acc 0.997\n",
      "epoch 4, loss 46.7020, train acc 0.998, test acc 0.997\n",
      "epoch 5, loss 37.3130, train acc 0.998, test acc 0.998\n",
      "Bias is 0.60\n",
      "epoch 1, loss 30996.4528, train acc 0.792, test acc 0.992\n",
      "epoch 2, loss 21594.5601, train acc 0.843, test acc 0.997\n",
      "epoch 3, loss 23100.7871, train acc 0.836, test acc 0.979\n",
      "epoch 4, loss 25577.7368, train acc 0.834, test acc 0.978\n",
      "epoch 5, loss 23101.4583, train acc 0.840, test acc 0.997\n",
      "Bias is 0.70\n",
      "epoch 1, loss 76956.1239, train acc 0.690, test acc 0.992\n",
      "epoch 2, loss 49859.8297, train acc 0.751, test acc 0.828\n",
      "epoch 3, loss 40790.5706, train acc 0.767, test acc 0.724\n",
      "epoch 4, loss 47837.0094, train acc 0.751, test acc 0.746\n",
      "epoch 5, loss 43235.6085, train acc 0.762, test acc 0.908\n",
      "Bias is 0.80\n",
      "epoch 1, loss 109146.0553, train acc 0.618, test acc 0.914\n",
      "epoch 2, loss 65690.5960, train acc 0.705, test acc 0.945\n",
      "epoch 3, loss 62079.8595, train acc 0.712, test acc 0.653\n",
      "epoch 4, loss 54894.8109, train acc 0.729, test acc 0.918\n",
      "epoch 5, loss 57794.2585, train acc 0.716, test acc 0.911\n",
      "Bias is 0.90\n",
      "epoch 1, loss 130238.8879, train acc 0.594, test acc 0.947\n",
      "epoch 2, loss 86029.5782, train acc 0.659, test acc 0.622\n",
      "epoch 3, loss 65359.8429, train acc 0.703, test acc 0.895\n",
      "epoch 4, loss 66310.5442, train acc 0.699, test acc 0.694\n",
      "epoch 5, loss 64420.8219, train acc 0.698, test acc 0.698\n",
      "Bias is 0.95\n",
      "epoch 1, loss 149601.7260, train acc 0.564, test acc 0.773\n",
      "epoch 2, loss 85627.8248, train acc 0.654, test acc 0.630\n",
      "epoch 3, loss 61292.6137, train acc 0.719, test acc 0.671\n",
      "epoch 4, loss 67321.7296, train acc 0.700, test acc 0.639\n",
      "epoch 5, loss 55648.1441, train acc 0.728, test acc 0.864\n"
     ]
    }
   ],
   "source": [
    "for i in lam:\n",
    "    train_shift, test_shift = generate_data(mnist_train, mnist_test,i, 6000)\n",
    "    print('Bias is %.2f' %i)\n",
    "    model(train_shift, test_shift, 256, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "1. If different size, $p(x)$ is the unbiased testing set. $q(x)$ is the biased training set. $\\beta(x)$ here is the weight.  \n",
    "$$\\int p(x)f(x)dx = \\int q(x)f(x)\\beta(x)dx $$ \n",
    "Weighting should be: $$\\beta(x_{class1}) = \\frac{p(x)}{q(x)} = \\frac{\\frac{N_{class=1}}{N_{test}}}{\\frac{N_{class=1}}{N_{trian}}} = \\frac{0.5}{0.1} = 5$$   \n",
    "$$\\beta(x_{class0}) = \\frac{p(x)}{q(x)} = \\frac{\\frac{N_{class=0}}{N_{test}}}{\\frac{N_{class=0}}{N_{trian}}} = \\frac{0.5}{0.9} = \\frac{5}{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give training set label 1 and testing set label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(ratio, batch_size):\n",
    "    mnist_train = gdata.vision.FashionMNIST(train=True,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    mnist_test = gdata.vision.FashionMNIST(train=False,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    bias_train, bias_test = generate_data(mnist_train, mnist_test,ratio, 6000)\n",
    "    #train's label should be 1 and test's label should be 0\n",
    "    f_train, l_train = bias_train[:]\n",
    "    f_test, l_test = bias_test[:]\n",
    "    \n",
    "    l_train = nd.ones((1, 12000)).astype(np.float32) \n",
    "    l_test = nd.zeros((1,4000)).astype(np.float32)\n",
    "    \n",
    "    trainLabel = nd.concat(l_train[:,:8000],l_test[:,:3000],dim = 1).reshape(shape=(-1,))\n",
    "    testLabel = nd.concat(l_train[:,8000:],l_test[:,3000:],dim = 1).reshape(shape=(-1,))\n",
    "    trainFeature = nd.concat(f_train[:8000], f_test[:3000], dim = 0)\n",
    "    testFeature = nd.concat(f_train[8000:], f_test[3000:], dim = 0)\n",
    "\n",
    "    trainDataset = gdata.dataset.ArrayDataset(trainFeature, trainLabel) \n",
    "    testDataset = gdata.dataset.ArrayDataset(testFeature, testLabel) \n",
    "    \n",
    "    train = gluon.data.DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "    test = gluon.data.DataLoader(testDataset, batch_size=batch_size, shuffle=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not very high accuracy for binary classification\n",
    "* Encoutered problem regrading to calculation of loss.When iterating, the loss became 0.0 for lambda = 0.1. Somehow, I stuck here and I use lambda = 0.2 to train the classification  \n",
    "* I understand the logic behind the covariate shift.   \n",
    "    1. There is only one output here and it would be class 1. I have already generating a dataset with data from training set labeled as 1, and data from testing set labeled as 0. The weight here should be 5 adding to the loss function.   \n",
    "    2. find the function $f$, which is trained by the Gluon net (a linear function) using the binary classification implemented in problem 2.   \n",
    "    3. Weigh training data using $$\\beta_i = min(\\exp(f(x_i)),c)$$    \n",
    "    and c would be a ceiling preventing the exponential function to be NAN   \n",
    "    4. Using the weights $\\beta_i$ for training on X with labels Y, which is problem 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logistic function and Loss function    \n",
    "reference from Gluon tutorial: Binary Classification with Logistic Regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1. + nd.exp(-z))\n",
    "\n",
    "def log_loss(output, y, ratio):\n",
    "    yhat = logistic(output)\n",
    "    yhat = yhat.reshape(shape=y.shape)\n",
    "    return  - nd.nansum(.5/ratio * y * nd.log(yhat) + .5/(1-ratio) * (1-y) * nd.log(1-yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, train_data, net, trainer, batch_size, ratio):\n",
    "    for e in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = log_loss(output, label, ratio)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "        print(\"Epoch %s, loss: %s\" % (e, cumulative_loss ))\n",
    "\n",
    "\n",
    "def test_model(test_data):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        num_total += len(label)\n",
    "        output = net(data)\n",
    "        prediction = ((nd.sign(output).reshape(shape=label.shape) + 1) / 2)\n",
    "        num_correct += nd.sum(prediction == label)\n",
    "    print(\"Accuracy: %0.3f (%s/%s)\" % (num_correct.asscalar()/num_total, num_correct.asscalar(), num_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 6826.333644866943\n",
      "Epoch 1, loss: 5480.05650138855\n",
      "Epoch 2, loss: 5035.953670501709\n",
      "Epoch 3, loss: 4794.918685913086\n",
      "Epoch 4, loss: 4639.720854759216\n",
      "Epoch 5, loss: 4526.004120826721\n",
      "Epoch 6, loss: 4443.828551292419\n",
      "Epoch 7, loss: 4380.837176322937\n",
      "Epoch 8, loss: 4325.9956884384155\n",
      "Epoch 9, loss: 4283.962030410767\n",
      "Epoch 10, loss: 4246.276669502258\n",
      "Epoch 11, loss: 4215.988206863403\n",
      "Epoch 12, loss: 4186.381437301636\n",
      "Epoch 13, loss: 4161.560842514038\n",
      "Epoch 14, loss: 4142.281621932983\n",
      "Epoch 15, loss: 4117.475589752197\n",
      "Epoch 16, loss: 4106.954483032227\n",
      "Epoch 17, loss: 4087.5182552337646\n",
      "Epoch 18, loss: 4076.430640220642\n",
      "Epoch 19, loss: 4062.342698097229\n",
      "Accuracy: 0.743 (2972.0/4000)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "total_per_label = 6000\n",
    "ratio = 0.2\n",
    "num_epochs = 20\n",
    "net = nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01))\n",
    "train_data, test_data = generator(ratio, batch_size)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.02})\n",
    "train_and_test_mnist_ratio(train_data, test_data, net, trainer, num_epochs, batch_size, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
