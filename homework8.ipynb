{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 8 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY, teammates A,B,C** (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/)**\n",
    "\n",
    "Handout 4/9/2019, due 4/16/2019 by 4pm.\n",
    "\n",
    "This homework deals with sequence models for text and numbers. Due to the computational cost, we strongly encourage you to implement this on a GPU enabled machine. To make things a bit more interesting we will use a larger text collection here - [Shakespeare's collected works](http://www.gutenberg.org/files/100/100-0.txt) which are freely downloadable at Project Gutenberg. \n",
    "\n",
    "**This is teamwork.**\n",
    "\n",
    "## Prerequisites - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters:  5032359\n",
      "project gutenberg s the complete works of william shakespeare by willi\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "import collections\n",
    "import re\n",
    "shakespeare = 'http://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "text = http.request('GET', shakespeare).data.decode('utf-8')\n",
    "raw_dataset = ' '.join(re.sub('[^A-Za-z]+', ' ', text).lower().split())\n",
    "\n",
    "print('number of characters: ', len(raw_dataset))\n",
    "print(raw_dataset[0:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is quite a bit bigger than the time machine (5 million vs. 160k). For convenience we also include the remaining preprocessing steps. A bigger dataset will allow us to generate more meaningful models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: project gutenberg s \n",
      "indices: [20, 6, 7, 18, 14, 12, 15, 9, 11, 13, 15, 14, 3, 4, 14, 6, 11, 9, 0, 9]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(raw_dataset))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)\n",
    "\n",
    "train_indices = corpus_indices[:-100000]\n",
    "test_indices = corpus_indices[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "['s', 'i', 'l', 'n', 'b', 'v', 'r', 'o', 'd', ' ', 'q', 'g', 'c', 'u', 'e', 't', 'h', 'f', 'j', 'z', 'p', 'k', 'w', 'x', 'm', 'a', 'y']\n",
      "{'s': 0, 'i': 1, 'l': 2, 'n': 3, 'b': 4, 'v': 5, 'r': 6, 'o': 7, 'd': 8, ' ': 9, 'q': 10, 'g': 11, 'c': 12, 'u': 13, 'e': 14, 't': 15, 'h': 16, 'f': 17, 'j': 18, 'z': 19, 'p': 20, 'k': 21, 'w': 22, 'x': 23, 'm': 24, 'a': 25, 'y': 26}\n",
      "5032359\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(idx_to_char)\n",
    "print(char_to_idx)\n",
    "print(len(corpus_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we import other useful libraries to help you getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Recurrent Latent Variable Models\n",
    "\n",
    "Train a number of different latent variable models using `train_indices` to assess their performance. By default pick 256 dimensions for the hidden units. You can use the codes provided in the class. Also, we strongly encourage you to use the Gluon implementation since it's a lot faster than building it from scratch.\n",
    "\n",
    "1. Train a single-layer RNN (with latent variables). \n",
    "1. Train a single-layer GRU.\n",
    "1. Train a single-layer LSTM. \n",
    "1. Train a two-layer LSTM. \n",
    "\n",
    "How low can you drive the perplexity? Can you reproduce some of Shakespeare's finest writing (generate 200 characters). Start the sequence generator with `But Brutus is an honorable man`. Experiment with a number of settings:\n",
    "\n",
    "* Number of hidden units. \n",
    "* Embedding length.\n",
    "* Gradient clipping.\n",
    "* Number of iterations. \n",
    "* Learning rate.\n",
    "\n",
    "**Save** the models (at least in memory since you'll need them in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "state = rnn_layer.begin_state(batch_size=batch_size)\n",
    "state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = rnn.GRU(num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, vocab_size)\n",
    "ctx = d2l.try_gpu()\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['but brutus is an honorable man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 3.543577, time 58.54 sec\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f63ed5d243a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                \u001b[0mcorpus_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipping_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                batch_size, pred_period, pred_len, prefixes)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\seebarsh\\miniconda3\\lib\\site-packages\\d2l\\utils.py\u001b[0m in \u001b[0;36mtrain_and_predict_rnn_gluon\u001b[1;34m(model, num_hiddens, vocab_size, ctx, corpus_indices, idx_to_char, char_to_idx, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001b[0m\n\u001b[0;32m    657\u001b[0m                 print(' -', predict_rnn_gluon(\n\u001b[0;32m    658\u001b[0m                     \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                     char_to_idx))\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\seebarsh\\miniconda3\\lib\\site-packages\\d2l\\utils.py\u001b[0m in \u001b[0;36mpredict_rnn_gluon\u001b[1;34m(prefix, num_chars, model, vocab_size, ctx, idx_to_char, char_to_idx)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;34m\"\"\"Precit next chars with a Gluon RNN model\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_chars\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'B'"
     ]
    }
   ],
   "source": [
    "d2l.train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                               corpus_indices, idx_to_char, char_to_idx,\n",
    "                               num_epochs, num_steps, lr, clipping_theta,\n",
    "                               batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "collapsed": true
   },
   "source": [
    "## 2. Test Error\n",
    "\n",
    "So far we measured perplexity only on the training set. \n",
    "\n",
    "1. Implement a perplexity calculator that does not involve training.\n",
    "1. Compute the perplexity of the best models in each of the 4 categories on the test set. By how much does it differ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $N$-Gram Model\n",
    "\n",
    "So far we only considered latent variable models. Let's see what happens if we use a regular $N$-gram model and an autoregressive setting. That is, we aim to predict the next character given the current characters one character at a time. For this implement the following:\n",
    "\n",
    "1. Split data into $(x,y)$ pairs as before, just that we now use very short subsequences, e.g. only $5$ characters. That is, `But Brutus` turns into the tuples (`(But B, r)`, `(ut Br, u)`, `(t Bru, t)`, `( Brut, u)`, `(Brutu, s)`). \n",
    "1. Use one-hot encoding for each character separately and combine them all. \n",
    "    * In one case use a sequential encoding to obtain an embedding proportional to the length of the sequence.\n",
    "    * Use a bag of characters encoding that sums over all occurrences.\n",
    "1. Implement an MLP with one hidden layer and 256 hidden units.\n",
    "1. Train it to output the next character.\n",
    "\n",
    "How accurate is the model? How does the number of operations and weights compare to an RNN, a GRU and an LSTM discussed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = 'But Brutus'\n",
    "idx_to_char = list(set(raw_dataset))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
    "#sample = corpus_indices[:20]\n",
    "#print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "#print('indices:', sample)\n",
    "\n",
    "#train_indices = corpus_indices[:-100000]\n",
    "#test_indices = corpus_indices[-100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 5 # embedding dimension for autoregressive model\n",
    "T = len(raw_dataset)\n",
    "features = nd.zeros((T-embedding, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(embedding):\n",
    "    features[:,i] = corpus_indices[i:T-embedding+i]\n",
    "labels = corpus_indices[embedding:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[20.  6.  7. 18. 14.]\n",
       " [ 6.  7. 18. 14. 12.]\n",
       " [ 7. 18. 14. 12. 15.]\n",
       " ...\n",
       " [22.  9. 14.  4.  7.]\n",
       " [ 9. 14.  4.  7.  7.]\n",
       " [14.  4.  7.  7. 21.]]\n",
       "<NDArray 5032354x5 @cpu(0)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = 5 # embedding dimension for autoregressive model\n",
    "T = len(raw_dataset)\n",
    "features = nd.zeros((T-embedding, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(embedding):\n",
    "    features[:,i] = corpus_indices[i:T-embedding+i]\n",
    "labels = corpus_indices[embedding:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot_length(X, vocab_size):\n",
    "    inputs = [nd.one_hot(x, vocab_size) for x in X]\n",
    "    matrix_size = inputs[0].size\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].reshape(1,matrix_size)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot_sum(X, vocab_size):\n",
    "    inputs = [nd.one_hot(x, vocab_size) for x in X]\n",
    "    matrix_size = inputs[0].size\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = nd.sum(inputs[i],axis = 0).reshape(1, vocab_size)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = to_onehot_length(features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " [[1. 0. 1. 2. 1. 0.]]\n",
       " <NDArray 1x6 @cpu(0)>, \n",
       " [[1. 0. 1. 1. 1. 1.]]\n",
       " <NDArray 1x6 @cpu(0)>, \n",
       " [[1. 0. 1. 1. 1. 1.]]\n",
       " <NDArray 1x6 @cpu(0)>, \n",
       " [[1. 0. 1. 1. 1. 1.]]\n",
       " <NDArray 1x6 @cpu(0)>, \n",
       " [[0. 0. 2. 1. 1. 1.]]\n",
       " <NDArray 1x6 @cpu(0)>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = to_onehot_sum(features, vocab_size)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(256, activation='relu'))\n",
    "net.add(gluon.nn.Dense(vocab_size))\n",
    "net.initialize(init.Xavier())\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 4932354\n",
    "train_data = gluon.data.ArrayDataset(features[:ntrain,:], labels[:ntrain])\n",
    "test_data  = gluon.data.ArrayDataset(features[ntrain:,:], labels[ntrain:])\n",
    "\n",
    "batch_size = 30\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "num_epochs = 10\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_iter = gluon.data.DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
