{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 8 - Berkeley STAT 157\n",
    "\n",
    "**Your name: XX, SID YY, teammates A,B,C** (Please add your name, SID and teammates to ease Ryan and Rachel to grade.)\n",
    "\n",
    "**Please submit your homework through [gradescope](http://gradescope.com/)**\n",
    "\n",
    "Handout 4/9/2019, due 4/16/2019 by 4pm.\n",
    "\n",
    "This homework deals with sequence models for text and numbers. Due to the computational cost, we strongly encourage you to implement this on a GPU enabled machine. To make things a bit more interesting we will use a larger text collection here - [Shakespeare's collected works](http://www.gutenberg.org/files/100/100-0.txt) which are freely downloadable at Project Gutenberg. \n",
    "\n",
    "**This is teamwork.**\n",
    "\n",
    "## Prerequisites - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters:  5032359\n",
      "project gutenberg s the complete works of william shakespeare by willi\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "import collections\n",
    "import re\n",
    "shakespeare = 'http://www.gutenberg.org/files/100/100-0.txt'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "text = http.request('GET', shakespeare).data.decode('utf-8')\n",
    "raw_dataset = ' '.join(re.sub('[^A-Za-z]+', ' ', text).lower().split())\n",
    "\n",
    "print('number of characters: ', len(raw_dataset))\n",
    "print(raw_dataset[0:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is quite a bit bigger than the time machine (5 million vs. 160k). For convenience we also include the remaining preprocessing steps. A bigger dataset will allow us to generate more meaningful models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: project gutenberg s \n",
      "indices: [21, 12, 3, 11, 7, 23, 9, 10, 17, 4, 9, 7, 19, 22, 7, 12, 17, 10, 24, 10]\n"
     ]
    }
   ],
   "source": [
    "idx_to_char = list(set(raw_dataset))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
    "sample = corpus_indices[:20]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)\n",
    "\n",
    "train_indices = corpus_indices[:-100000]\n",
    "test_indices = corpus_indices[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "['k', 'y', 'w', 'o', 'u', 'i', 'a', 'e', 'q', 't', ' ', 'j', 'r', 'v', 'f', 'z', 'm', 'g', 'd', 'n', 'x', 'p', 'b', 'c', 's', 'h', 'l']\n",
      "{'k': 0, 'y': 1, 'w': 2, 'o': 3, 'u': 4, 'i': 5, 'a': 6, 'e': 7, 'q': 8, 't': 9, ' ': 10, 'j': 11, 'r': 12, 'v': 13, 'f': 14, 'z': 15, 'm': 16, 'g': 17, 'd': 18, 'n': 19, 'x': 20, 'p': 21, 'b': 22, 'c': 23, 's': 24, 'h': 25, 'l': 26}\n",
      "5032359\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(idx_to_char)\n",
    "print(char_to_idx)\n",
    "print(len(corpus_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we import other useful libraries to help you getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import time\n",
    "import mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $N$-Gram Model\n",
    "\n",
    "So far we only considered latent variable models. Let's see what happens if we use a regular $N$-gram model and an autoregressive setting. That is, we aim to predict the next character given the current characters one character at a time. For this implement the following:\n",
    "\n",
    "1. Split data into $(x,y)$ pairs as before, just that we now use very short subsequences, e.g. only $5$ characters. That is, `But Brutus` turns into the tuples (`(But B, r)`, `(ut Br, u)`, `(t Bru, t)`, `( Brut, u)`, `(Brutu, s)`). \n",
    "1. Use one-hot encoding for each character separately and combine them all. \n",
    "    * In one case use a sequential encoding to obtain an embedding proportional to the length of the sequence.\n",
    "    * Use a bag of characters encoding that sums over all occurrences.\n",
    "1. Implement an MLP with one hidden layer and 256 hidden units.\n",
    "1. Train it to output the next character.\n",
    "\n",
    "How accurate is the model? How does the number of operations and weights compare to an RNN, a GRU and an LSTM discussed above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = d2l.try_gpu()\n",
    "embedding = 5 # embedding dimension for autoregressive model\n",
    "T = len(raw_dataset)\n",
    "features = nd.zeros((T-embedding, embedding), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(embedding):\n",
    "    features[:,i] = corpus_indices[i:T-embedding+i]\n",
    "labels = nd.array(corpus_indices[embedding:],ctx =ctx).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[20.  6.  7. 18. 14.]\n",
       " [ 6.  7. 18. 14. 12.]\n",
       " [ 7. 18. 14. 12. 15.]\n",
       " ...\n",
       " [22.  9. 14.  4.  7.]\n",
       " [ 9. 14.  4.  7.  7.]\n",
       " [14.  4.  7.  7. 21.]]\n",
       "<NDArray 5032354x5 @cpu(0)>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[12.]\n",
       " [15.]\n",
       " [ 9.]\n",
       " ...\n",
       " [ 7.]\n",
       " [21.]\n",
       " [ 0.]]\n",
       "<NDArray 5032354x1 @cpu(0)>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = 'but brutus is an honorable man'\n",
    "corpus_indices_predict = [char_to_idx[char] for char in predict_dataset]\n",
    "T_predict = len(predict_dataset)\n",
    "predict_features = nd.zeros((T_predict-embedding, embedding),ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(embedding):\n",
    "    predict_features[:,i] = corpus_indices_predict[i:T_predict-embedding+i]\n",
    "predict_labels = nd.array(corpus_indices_predict[embedding:],ctx = ctx).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[22.  4.  9. 10. 22.]\n",
       " [ 4.  9. 10. 22. 12.]\n",
       " [ 9. 10. 22. 12.  4.]\n",
       " [10. 22. 12.  4.  9.]\n",
       " [22. 12.  4.  9.  4.]\n",
       " [12.  4.  9.  4. 24.]\n",
       " [ 4.  9.  4. 24. 10.]\n",
       " [ 9.  4. 24. 10.  5.]\n",
       " [ 4. 24. 10.  5. 24.]\n",
       " [24. 10.  5. 24. 10.]\n",
       " [10.  5. 24. 10.  6.]\n",
       " [ 5. 24. 10.  6. 19.]\n",
       " [24. 10.  6. 19. 10.]\n",
       " [10.  6. 19. 10. 25.]\n",
       " [ 6. 19. 10. 25.  3.]\n",
       " [19. 10. 25.  3. 19.]\n",
       " [10. 25.  3. 19.  3.]\n",
       " [25.  3. 19.  3. 12.]\n",
       " [ 3. 19.  3. 12.  6.]\n",
       " [19.  3. 12.  6. 22.]\n",
       " [ 3. 12.  6. 22. 26.]\n",
       " [12.  6. 22. 26.  7.]\n",
       " [ 6. 22. 26.  7. 10.]\n",
       " [22. 26.  7. 10. 16.]\n",
       " [26.  7. 10. 16.  6.]]\n",
       "<NDArray 25x5 @gpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[12.]\n",
       " [ 4.]\n",
       " [ 9.]\n",
       " [ 4.]\n",
       " [24.]\n",
       " [10.]\n",
       " [ 5.]\n",
       " [24.]\n",
       " [10.]\n",
       " [ 6.]\n",
       " [19.]\n",
       " [10.]\n",
       " [25.]\n",
       " [ 3.]\n",
       " [19.]\n",
       " [ 3.]\n",
       " [12.]\n",
       " [ 6.]\n",
       " [22.]\n",
       " [26.]\n",
       " [ 7.]\n",
       " [10.]\n",
       " [16.]\n",
       " [ 6.]\n",
       " [19.]]\n",
       "<NDArray 25x1 @gpu(0)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot_length(X, vocab_size):\n",
    "    inputs = [nd.one_hot(x, vocab_size) for x in X]\n",
    "    matrix_size = inputs[0].size\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].reshape(1,matrix_size)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def to_onehot_output(Y, vocab_size):\n",
    "    #outputs = [nd.one_hot(y, vocab_size) for y in Y]\n",
    "    #return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot_sum(X, vocab_size):\n",
    "    inputs = [nd.one_hot(x, vocab_size) for x in X]\n",
    "    matrix_size = inputs[0].size\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = nd.sum(inputs[i],axis = 0).reshape(1, vocab_size)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这俩不能一起跑，内存会炸\n",
    "inputs = to_onehot_length(features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sum = to_onehot_sum(features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#predictIn = to_onehot_length(predict_features, vocab_size)\n",
    "#print('finished')\n",
    "#predictInSum = to_onehot_sum(predict_features, vocab_size)\n",
    "#print('finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "net.add(gluon.nn.Dense(256, activation='relu'))\n",
    "net.add(gluon.nn.Dense(vocab_size))\n",
    "net.initialize(init.Xavier())\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 900000\n",
    "train_data = gluon.data.ArrayDataset(inputs[:ntrain], labels[:ntrain])\n",
    "test_data  = gluon.data.ArrayDataset(inputs[ntrain:1000000], labels[ntrain:1000000])\n",
    "\n",
    "batch_size = 30\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.8})\n",
    "num_epochs = 5 \n",
    "\n",
    "train_iter = gluon.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_iter = gluon.data.DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for second encoding version\n",
    "ntrain = 900000\n",
    "train_data_2 = gluon.data.ArrayDataset(inputs_sum[:ntrain], labels[:ntrain])\n",
    "test_data_2  = gluon.data.ArrayDataset(inputs_sum[ntrain:1000000], labels[ntrain:1000000])\n",
    "\n",
    "batch_size = 30\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "num_epochs = 5\n",
    "\n",
    "train_iter_2 = gluon.data.DataLoader(train_data_2, batch_size, shuffle=True)\n",
    "test_iter_2 = gluon.data.DataLoader(test_data_2, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is saved in the d2l package for future use\n",
    "def predict_ngram(prefix, num_chars, net, params, features,\n",
    "                num_hiddens, vocab_size, ctx):\n",
    "    \n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    \n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # The output of the previous time step is taken as the input of the\n",
    "        # current time step.\n",
    "        \n",
    "        # Calculate the output and update the hidden state\n",
    "        Y = net(X)\n",
    "        # The input to the next time step is the character in the prefix or\n",
    "        # the current best predicted character\n",
    "        if t < len(prefix) - 1:\n",
    "            # Read off from the given sequence of characters\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            # This is maximum likelihood decoding. Modify this if you want\n",
    "            # use sampling, beam search or beam sampling for better sequences.\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "            \n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'but brutus is an honorable man'\n",
    "output = [char_to_idx[prefix[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
